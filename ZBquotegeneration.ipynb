{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Zx7L5X_Wqqn",
        "outputId": "bac243e2-1885-4002-f109-b1e3338f0001"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 64])\n",
            "\n",
            "--- Starting epoch #0 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1407/1407 [08:05<00:00,  2.90it/s]\n",
            "Validation: 100%|██████████| 79/79 [00:17<00:00,  4.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch #0 finished --- Training cost: 0.18449879344767994 / Validation cost: 0.020559473633766173\n",
            "\n",
            "--- Starting epoch #1 ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 1407/1407 [08:08<00:00,  2.88it/s]\n",
            "Validation: 100%|██████████| 79/79 [00:17<00:00,  4.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Epoch #1 finished --- Training cost: 0.04173726387065318 / Validation cost: 0.015725482780486345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import csv\n",
        "import json\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import json\n",
        "import torch.nn.functional as F\n",
        "from tqdm import trange\n",
        "\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the path to your JSON file\n",
        "json_file_path = '/content/drive/MyDrive/Quote-Generation-ML/data/quotes.json'\n",
        "\n",
        "def load_quotes(num_quotes):\n",
        "    # Load the JSON file and select the first `num_quotes` quotes\n",
        "    with open(json_file_path, 'r') as file:\n",
        "        quotes_data = json.load(file)\n",
        "\n",
        "    # Select the first `num_quotes` quotes\n",
        "    quotes_data = quotes_data[:num_quotes]\n",
        "\n",
        "    # Extract quotes and categories from the dataset\n",
        "    quotes = [entry['quote'] for entry in quotes_data]\n",
        "    categories = [entry['categories'] for entry in quotes_data]\n",
        "\n",
        "    # Combine quotes and categories into a single input for the model\n",
        "    inputs = [f\"{quote} Categories: {', '.join(category)}\" for quote, category in zip(quotes, categories)]\n",
        "\n",
        "    return inputs\n",
        "\n",
        "# Set up GPT-2 model and tokenizer\n",
        "MODEL_NAME = 'distilgpt2'  # 'distilgpt2' or 'gpt2-medium'\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
        "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Declare special tokens for padding and separating the context from the quote:\n",
        "SPECIAL_TOKENS_DICT = {\n",
        "    'pad_token': '<pad>',\n",
        "    'additional_special_tokens': ['<context>', '<quote>'],\n",
        "}\n",
        "\n",
        "# Add these special tokens to the vocabulary and resize model's embeddings:\n",
        "tokenizer.add_special_tokens(SPECIAL_TOKENS_DICT)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Create a custom dataset for quote generation\n",
        "class QuoteDataset(Dataset):\n",
        "    def __init__(self, quotes, tokenizer, seq_length=64):\n",
        "        context_tkn = tokenizer.additional_special_tokens_ids[0]\n",
        "        quote_tkn = tokenizer.additional_special_tokens_ids[1]\n",
        "        pad_tkn = tokenizer.pad_token_id\n",
        "        eos_tkn = tokenizer.eos_token_id\n",
        "\n",
        "        self.examples = []\n",
        "        for quote in quotes:\n",
        "            # Build the context and quote segments:\n",
        "            context = [context_tkn] + tokenizer.encode(quote, max_length=seq_length // 2 - 1)\n",
        "            quote = [quote_tkn] + tokenizer.encode(quote, max_length=seq_length // 2 - 2) + [eos_tkn]\n",
        "\n",
        "            # Concatenate the two parts together:\n",
        "            tokens = context + quote + [pad_tkn] * (seq_length - len(context) - len(quote))\n",
        "\n",
        "            # Annotate each token with its corresponding segment:\n",
        "            segments = [context_tkn] * len(context) + [quote_tkn] * (seq_length - len(context))\n",
        "\n",
        "            # Ignore the context, padding, and <quote> tokens by setting their labels to -100\n",
        "            labels = [-100] * (len(context) + 1) + quote[1:] + [-100] * (seq_length - len(context) - len(quote))\n",
        "\n",
        "            # Add the preprocessed example to the dataset\n",
        "            self.examples.append((tokens, segments, labels))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return torch.tensor(self.examples[item])\n",
        "\n",
        "def fit(model, optimizer, train_dl, val_dl, epochs=1, device=torch.device('cpu')):\n",
        "    for i in range(epochs):\n",
        "        print('\\n--- Starting epoch #{} ---'.format(i))\n",
        "        model.train()\n",
        "        losses = []\n",
        "        nums = []\n",
        "\n",
        "        for batch in tqdm(train_dl, desc=\"Training\"):\n",
        "            inputs = batch.to(device)\n",
        "            outputs = model(inputs[:, 0, :], token_type_ids=inputs[:, 1, :], labels=inputs[:, 2, :])\n",
        "            loss = outputs[0]\n",
        "            losses.append(loss.item())\n",
        "            nums.append(len(batch))\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            model.zero_grad()\n",
        "\n",
        "        train_cost = np.sum(np.multiply(losses, nums)) / sum(nums)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            losses = []\n",
        "            nums = []\n",
        "\n",
        "            for batch in tqdm(val_dl, desc=\"Validation\"):\n",
        "                inputs = batch.to(device)\n",
        "                outputs = model(inputs[:, 0, :], token_type_ids=inputs[:, 1, :], labels=inputs[:, 2, :])\n",
        "                losses.append(outputs[0].item())\n",
        "                nums.append(len(batch))\n",
        "\n",
        "        val_cost = np.sum(np.multiply(losses, nums)) / sum(nums)\n",
        "\n",
        "        print('\\n--- Epoch #{} finished --- Training cost: {} / Validation cost: {}'.format(i, train_cost, val_cost))\n",
        "\n",
        "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
        "    top_k = min(top_k, logits.size(-1))\n",
        "    if top_k > 0:\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p > 0.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits\n",
        "\n",
        "def sample_sequence(model, length, context, segments_tokens=None, num_samples=1, temperature=1, top_k=0, top_p=0.0, repetition_penalty=1.0, device='cpu'):\n",
        "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
        "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
        "    generated = context\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in trange(length):\n",
        "            inputs = {'input_ids': generated}\n",
        "            if segments_tokens is not None:\n",
        "                inputs['token_type_ids'] = torch.tensor(segments_tokens[:generated.shape[1]]).unsqueeze(0).repeat(num_samples, 1)\n",
        "\n",
        "            outputs = model(**inputs)\n",
        "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\n",
        "\n",
        "            for i in range(num_samples):\n",
        "                for _ in set(generated[i].tolist()):\n",
        "                    next_token_logits[i, _] /= repetition_penalty\n",
        "\n",
        "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
        "            if temperature == 0:\n",
        "                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\n",
        "            else:\n",
        "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
        "            generated = torch.cat((generated, next_token), dim=1)\n",
        "    return generated\n",
        "\n",
        "\n",
        "# Number of quotes to load\n",
        "num_quotes_to_load = 50000  # Change this number as needed\n",
        "\n",
        "# Load quotes using the function\n",
        "inputs = load_quotes(num_quotes_to_load)\n",
        "\n",
        "# Build the dataset and display the dimensions of the 1st batch for verification:\n",
        "quote_dataset = QuoteDataset(inputs, tokenizer)\n",
        "print(next(iter(quote_dataset)).size())\n",
        "\n",
        "# Create data indices for training and validation splits:\n",
        "indices = list(range(len(quote_dataset)))\n",
        "random.seed(42)\n",
        "random.shuffle(indices)\n",
        "split = math.floor(0.1 * len(quote_dataset))\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "# Build the PyTorch data loaders:\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "val_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "train_loader = DataLoader(quote_dataset, batch_size=32, sampler=train_sampler)\n",
        "val_loader = DataLoader(quote_dataset, batch_size=64, sampler=val_sampler)\n",
        "\n",
        "# Move the model to the GPU:\n",
        "device = torch.device('cuda')\n",
        "model.to(device)\n",
        "\n",
        "# Fine-tune GPT2 for two epochs:\n",
        "optimizer = AdamW(model.parameters())\n",
        "fit(model, optimizer, train_loader, val_loader, epochs=2, device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Sample quotes using the trained model\n",
        "context = \"The purpose of\"\n",
        "\n",
        "context_tkn = tokenizer.additional_special_tokens_ids[0]\n",
        "quote_tkn = tokenizer.additional_special_tokens_ids[1]\n",
        "\n",
        "input_ids = [context_tkn] + tokenizer.encode(context)\n",
        "\n",
        "segments = [quote_tkn] * 64\n",
        "segments[:len(input_ids)] = [context_tkn] * len(input_ids)\n",
        "\n",
        "input_ids += [quote_tkn]\n",
        "\n",
        "# Move the model back to the CPU for inference:\n",
        "model.to(torch.device('cpu'))\n",
        "\n",
        "# Generate 10 samples of max length 20 with a higher repetition penalty\n",
        "generated = sample_sequence(model, length=20, context=input_ids, segments_tokens=segments, num_samples=10, repetition_penalty=2.0)\n",
        "\n",
        "print('\\n\\n--- Generated Quotes ---\\n')\n",
        "\n",
        "for g in generated:\n",
        "    quote = tokenizer.decode(g.squeeze().tolist())\n",
        "    print(quote)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8K_pHuTChuRi",
        "outputId": "e2392337-ad63-44a9-f213-5fc82eee9c74"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:11<00:00,  1.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "--- Generated Quotes ---\n",
            "\n",
            "<quote> The purpose of <context> the purposeian,manuel Houston Rockets Rockets Bullets bullets arrows ink tags hand markings flash snow clipped talk numb\n",
            "<quote> The purpose of <context> the purposeful <quote>  results from September September autumn autumn Reynolds •• Sketchz z ZZzn Zombie\n",
            "<quote> The purpose of <context> the purpose about<|endoftext|>people different from Timeline to make forever forever forever forever forever, Mama Mom booted shut\n",
            "<quote> The purpose of <context> the purpose for<|endoftext|>that means meant meaning meaning meaning reason which later next season Conference instructswards Mayo\n",
            "<quote> The purpose of <context> the reason why why why why why why why why why why why why why why why why Why why\n",
            "<quote> The purpose of <context> the purpose, between both either either neither neither neither neither either neither neither neither misunderstand pity if before then\n",
            "<quote> The purpose of <context> the purpose most important <context> <|endoftext|>this so very close this condition, otherwise forbiddenbidden unre certain sensitive hobbies\n",
            "<quote> The purpose of <context> the purpose is only with with who who sk but which has these cafes coffee Café café cafe Cafe Caf\n",
            "<quote> The purpose of <context> the purpose for<|endoftext|>will for last last<|endoftext|> will give only give your gift instead let the hand to\n",
            "<quote> The purpose of <context> the purposeof<|endoftext|>Original<|endoftext|> Categories: Categories: Categories.\n",
            " ; know G notegi you\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}